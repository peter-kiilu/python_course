{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peter-kiilu/python_course/blob/main/Data_Pre_processing_using_pandas_and_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson: Data Cleaning, Wrangling, Transformation, and Aggregation using Pandas & NumPy\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Data cleaning, wrangling, transformation, and aggregation are crucial steps in the data analysis pipeline. They help in preparing data for meaningful insights by handling missing values, inconsistencies, and structuring it for analysis. This lesson will use **Pandas** and **NumPy** to demonstrate these processes using real-world datasets from Kaggle.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Basic knowledge of Python.\n",
        "- Understanding of Pandas and NumPy fundamentals.\n",
        "- Installation of Pandas and NumPy (`pip install pandas numpy`).\n",
        "\n",
        "## Dataset Selection\n",
        "\n",
        "For this lesson, we will use the **\"House Prices - Advanced Regression Techniques\"** dataset from Kaggle. It contains real estate data with various attributes about houses (e.g., location, size, price, etc.).\n",
        "\n",
        "Download the dataset from Kaggle: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Loading and Exploring Data\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Display basic information\n",
        "df.info()\n",
        "\n",
        "# Show the first few rows\n",
        "df.head()\n",
        "\n",
        "# Checking for duplicate entries\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "```\n",
        "\n",
        "### Additional Example:\n",
        "\n",
        "```python\n",
        "# Summary statistics\n",
        "df.describe()\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- `df.info()` provides an overview of columns, data types, and missing values.\n",
        "- `df.head()` displays the first five rows of the dataset to get an initial look at the data.\n",
        "- `df.duplicated().sum()` helps identify duplicate records, which can cause bias in analysis.\n",
        "- `df.describe()` provides summary statistics of numerical columns, helping to understand the distribution of data.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Handling Missing Values\n",
        "\n",
        "### Identifying Missing Data\n",
        "\n",
        "```python\n",
        "# Count missing values per column\n",
        "df.isnull().sum().sort_values(ascending=False)\n",
        "```\n",
        "\n",
        "### Dropping Columns with Too Many Missing Values\n",
        "\n",
        "```python\n",
        "# Drop columns where more than 40% of values are missing\n",
        "df = df.dropna(thresh=0.6*len(df), axis=1)\n",
        "```\n",
        "\n",
        "### Filling Missing Values\n",
        "\n",
        "```python\n",
        "# Fill numerical columns with median\n",
        "df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].median(), inplace=True)\n",
        "\n",
        "# Fill categorical columns with the mode\n",
        "df[\"Electrical\"].fillna(df[\"Electrical\"].mode()[0], inplace=True)\n",
        "```\n",
        "\n",
        "### Additional Example:\n",
        "\n",
        "```python\n",
        "# Fill missing values in all categorical columns with 'Unknown'\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_columns] = df[categorical_columns].fillna('Unknown')\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **Identifying missing values** helps in understanding which columns need attention.\n",
        "- **Dropping columns** with too many missing values prevents incomplete data from affecting analysis.\n",
        "- **Filling missing values** ensures data completeness. The median is used for numerical columns to avoid skewing data, while the mode (most frequent value) or 'Unknown' is used for categorical data.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Data Transformation\n",
        "\n",
        "### Changing Data Types\n",
        "\n",
        "```python\n",
        "# Convert column data types\n",
        "df[\"MSSubClass\"] = df[\"MSSubClass\"].astype(str)\n",
        "df[\"YearBuilt\"] = pd.to_datetime(df[\"YearBuilt\"], format='%Y')\n",
        "```\n",
        "\n",
        "### Normalization and Scaling\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[[\"GrLivArea\", \"SalePrice\"]] = scaler.fit_transform(df[[\"GrLivArea\", \"SalePrice\"]])\n",
        "```\n",
        "\n",
        "### Additional Example:\n",
        "\n",
        "```python\n",
        "# Standardization using StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[[\"GrLivArea\", \"SalePrice\"]] = scaler.fit_transform(df[[\"GrLivArea\", \"SalePrice\"]])\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **Data type conversion** ensures that categorical features are not mistakenly treated as numerical.\n",
        "- **Normalization (MinMaxScaler)** scales values between 0 and 1, ensuring fair comparisons.\n",
        "- **Standardization (StandardScaler)** centers the distribution around zero, which is useful for models that assume normally distributed data.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Feature Engineering\n",
        "\n",
        "### Creating New Features\n",
        "\n",
        "```python\n",
        "# Creating a new feature: House Age\n",
        "df[\"HouseAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"].dt.year\n",
        "```\n",
        "\n",
        "### Binning Continuous Variables\n",
        "\n",
        "```python\n",
        "# Binning SalePrice into categories\n",
        "df[\"PriceCategory\"] = pd.cut(df[\"SalePrice\"], bins=[0, 0.3, 0.6, 1.0], labels=[\"Low\", \"Medium\", \"High\"])\n",
        "```\n",
        "\n",
        "### Additional Example:\n",
        "\n",
        "```python\n",
        "# Creating an interaction feature\n",
        "df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **Feature engineering** enhances dataset quality.\n",
        "- `HouseAge` helps analyze property condition.\n",
        "- `PriceCategory` groups continuous values into meaningful categories for better understanding.\n",
        "- `TotalSF` creates a useful feature by combining multiple square footage values.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Data Wrangling & Aggregation\n",
        "\n",
        "### Grouping Data\n",
        "\n",
        "```python\n",
        "# Aggregating average price per neighborhood\n",
        "neighborhood_prices = df.groupby(\"Neighborhood\")[\"SalePrice\"].mean().reset_index()\n",
        "```\n",
        "\n",
        "### Pivot Tables\n",
        "\n",
        "```python\n",
        "# Creating a pivot table to analyze mean price by house style\n",
        "pivot = df.pivot_table(values=\"SalePrice\", index=\"HouseStyle\", aggfunc=np.mean)\n",
        "```\n",
        "\n",
        "### Additional Example:\n",
        "\n",
        "```python\n",
        "# Grouping by multiple columns\n",
        "df.groupby([\"Neighborhood\", \"HouseStyle\"])[\"SalePrice\"].mean()\n",
        "```\n",
        "\n",
        "### Merging DataFrames\n",
        "\n",
        "```python\n",
        "# Assume we have an additional dataset with crime rates\n",
        "crime_data = pd.read_csv(\"crime_rates.csv\")\n",
        "df = df.merge(crime_data, on=\"Neighborhood\", how=\"left\")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **Grouping data** helps extract insights by summarizing key metrics.\n",
        "- **Pivot tables** provide an Excel-like method to analyze structured data.\n",
        "- **Merging datasets** integrates additional information for deeper analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Exporting Cleaned Data\n",
        "\n",
        "```python\n",
        "# Save the cleaned dataset\n",
        "df.to_csv(\"cleaned_house_prices.csv\", index=False)\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "- **Exporting** allows saving processed data for future use and sharing.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Data cleaning and transformation are essential steps before analysis or modeling. Using Pandas and NumPy, we handled missing values, transformed data types, engineered new features, and aggregated insights. These techniques are crucial in real-world applications like real estate price prediction and customer analytics.\n",
        "\n",
        "---\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Load another dataset from Kaggle and identify missing values.\n",
        "2. Implement feature engineering techniques on a dataset of your choice.\n",
        "3. Aggregate data by a categorical column and visualize the results using Seaborn.\n",
        "4. Merge two different datasets and analyze the impact on the final dataset.\n",
        "\n",
        "By applying these skills, you'll be well-prepared for data cleaning and transformation in real-world scenarios!\n",
        "\n"
      ],
      "metadata": {
        "id": "Cbk6Vq2U_I3N"
      }
    }
  ]
}